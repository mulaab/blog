<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title> - @M0ula__ab</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="" />
<meta property="og:description" content="Pra proses data data Science https://luthfan.com/cara-meningkatkan-daya-ingat-otak-menurut-islam/
Analisa data pada data sain dimulai dengan suatu proses dimana mesin belajar dari data pelatihan(training set), sehingga jika data pelatihan mengandung data kotor maka informasi yang diperoleh adalah informasi yang kotor. Artinya kesimpulan dari analisa lemah. Oleh karena itu mengolah data mentah yang dikumpulkan untuk di proses lebih dahulu agar menjadi data yang bersih sangat penting dalam analisa data. Kualitas data dari data training sangat penting dalam menghasilkan analis suatu data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mulaab.github.io/blog/post/praproses-data/" />


	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="https://mulaab.github.io/blog/css/style.css">
	
	<link rel="shortcut icon" href="https://mulaab.github.io/blog/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="https://mulaab.github.io/blog/" title="@M0ula__ab" rel="home">
				<div class="logo__title">@M0ula__ab</div>
				<div class="logo__tagline">Only Share it</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="https://mulaab.github.io/blog/post/200519-migrate-from-jekyll/">Jekyll migration</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://mulaab.github.io/blog/post/190519-hugois/">(Hu)go Template Primer</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://mulaab.github.io/blog/about/">Diriku</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://mulaab.github.io/blog/post/hugoisforlovers/">Getting Started with Hugo</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://mulaab.github.io/blog/regresi-linier/">notebook</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title"></h1>
			
		</header><div class="content post__content clearfix">
			

<h1 id="pra-proses-data-data-science">Pra proses data   data Science</h1>

<p><a href="https://luthfan.com/cara-meningkatkan-daya-ingat-otak-menurut-islam/">https://luthfan.com/cara-meningkatkan-daya-ingat-otak-menurut-islam/</a></p>

<p>Analisa data pada data sain dimulai dengan suatu proses dimana mesin belajar dari data pelatihan(training set), sehingga jika data pelatihan mengandung data kotor maka informasi yang diperoleh adalah informasi yang kotor. Artinya kesimpulan dari analisa lemah. Oleh karena itu mengolah data mentah yang dikumpulkan untuk di proses lebih dahulu agar menjadi data yang bersih sangat penting dalam analisa data. Kualitas data dari data training sangat penting dalam menghasilkan analis suatu data. Pra proses data dalam rangkain pembelajaran mesin dalam data sain merupakan tahapan awal dari keseluruhan proses data sain. Seperti pada gambar berikut</p>

<p><img src="./images/bagan.jpg" width=90%></p>

<h3 id="overview">Overview</h3>

<ul>
<li><a href="#Dealing-with-missing-data">Dealing with missing data</a>

<ul>
<li><a href="#Eliminating-samples-or-features-with-missing-values">Eliminating samples or features with missing values</a></li>
<li><a href="#Imputing-missing-values">Imputing missing values</a></li>
<li><a href="#Understanding-the-scikit-learn-estimator-API">Understanding the scikit-learn estimator API</a></li>
</ul></li>
<li><a href="#Handling-categorical-data">Handling categorical data</a>

<ul>
<li><a href="#Mapping-ordinal-features">Mapping ordinal features</a></li>
<li><a href="#Encoding-class-labels">Encoding class labels</a></li>
<li><a href="#Performing-one-hot-encoding-on-nominal-features">Performing one-hot encoding on nominal features</a></li>
</ul></li>
<li><a href="#Partitioning-a-dataset-in-training-and-test-sets">Partitioning a dataset in training and test sets</a></li>
<li><a href="#Bringing-features-onto-the-same-scale">Bringing features onto the same scale</a></li>
<li><a href="#Selecting-meaningful-features">Selecting meaningful features</a>

<ul>
<li><a href="#Sparse-solutions-with-L1-regularization">Sparse solutions with L1 regularization</a></li>
<li><a href="#Sequential-feature-selection-algorithms">Sequential feature selection algorithms</a></li>
</ul></li>
<li><a href="#Assessing-feature-importance-with-random-forests">Assessing feature importance with random forests</a></li>
<li><a href="#Summary">Summary</a></li>
</ul>

<h1 id="praproses-data">Praproses Data</h1>

<p>Dalam data sain pra proses data dapat dilakukan dengan beberapa macam</p>

<ol>
<li><p>Menyelesaikan data hilang ( Missing Values)</p></li>

<li><p>Seleksi Fitur (Feature Selection)</p></li>

<li><p>Ektraksi Fitur (Feature Extraction)</p></li>

<li><p>Transformasi Data</p></li>

<li><p>Penskalaan Data</p></li>
</ol>

<p><a href="https://medium.com/datadriveninvestor/finding-outliers-in-dataset-using-python-efc3fce6ce32">https://medium.com/datadriveninvestor/finding-outliers-in-dataset-using-python-efc3fce6ce32</a> mencari outlier</p>

<h2 id="menyelesain-data-hilang">Menyelesain Data Hilang</h2>

<p>Menyelesain data hilang dilakukan dengan cara :
- Mengeliminasi sampel atau fitur dari data yang hilang
- Mengimputan nilai yang hilang</p>

<p>Data yang hilang sebabkan oleh bebera hal diantaranya</p>

<ul>
<li>Kesalahan dalam mengumpulkan data</li>
<li>Pengukuran alat tidak befungsi
Banyak algoritma pembelajaran mesin tidak handal jika ada data yang hilang. Oleh karena itu kita perlu melakukan proses penyelesaian data hilang sebelum melakukan pelatihan model.</li>
</ul>

<p>Kita gunakan pandas ( library dari python untuk analisa data) terkiat dengan data hilang seperti contoh berikut :</p>

<pre><code class="language-python">from IPython.display import Image

%matplotlib inline
# Added version check for recent scikit-learn 0.18 checks
from distutils.version import LooseVersion as Version
from sklearn import __version__ as sklearn_version

</code></pre>

<pre><code class="language-python">import pandas as pd
from io import StringIO

csv_data = '''A,B,C,D
1.0,2.0,3.0,4.0
5.0,,,8.0
10.0,,12.0,'''

# Jika anda menggunakan Python 2.7, anda perlu 
# untuk konversi string ke unicode:
# csv_data = unicode(csv_data)

df = pd.read_csv(StringIO(csv_data))
df
</code></pre>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)

&lt;ipython-input-8-8686f6354da8&gt; in &lt;module&gt;()
     11 # csv_data = unicode(csv_data)
     12 
---&gt; 13 df = pd.read_csv(StringIO(csv_data))
     14 df


TypeError: initial_value must be unicode or None, not str
</code></pre>

<pre><code class="language-python">Ada 4 kolom sebagai fitur A, B, C, D

Baris 0, 1, 2 adalah sampel-sample data (objek).

NaN adalah tanda bahwa nilai hilang (Missing values), tidak ada bilang bilangan yang diinputkan
</code></pre>

<pre><code>  File &quot;&lt;ipython-input-3-01b1e7e37d99&gt;&quot;, line 1
    Ada 4 kolom sebagai fitur A, B, C, D
        ^
SyntaxError: invalid syntax
</code></pre>

<p>Perintah berikut untuk menyatakan bahwa  jika cell tersebut adalah nilai hilang makan bernilai</p>

<pre><code class="language-python">df.isnull()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>False</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>

<p>Perintah berikut untuk menyatakan jumlah data yang hilang pada fitur(kolom) tertentu</p>

<pre><code class="language-python">df.isnull().sum(axis=0)
</code></pre>

<pre><code>A    0
B    2
C    1
D    1
dtype: int64
</code></pre>

<h2 id="menghilang-sample-dimana-ada-fitur-yang-nilainya-hilang-missing-values">Menghilang sample dimana ada fitur yang nilainya hilang (missing values)</h2>

<p>Salah satu strategi sederhananya adalah menghilangkan sample ( baris dalam tabel) atau fitur ( kolom dari tabel) dimana terdapat nilai hilang (missing values) didasarkan beberapa kriteria</p>

<pre><code class="language-python"># menghilangkan sample atau baris
df.dropna()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Menghilang fitur atau kolomg
df.dropna(axis=1)
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># hanya menghilang baris dimana semua kolom/fitur adalah NaN
df.dropna(how='all')  
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.0</td>
      <td>NaN</td>
      <td>12.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Menhgilangkan baris yang memiliki paling sedikit mangandung 4 fitur yang tidak NaN 
df.dropna(thresh=4)
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># hanya menghilangkan baris dimana  NaN  muncul pada kolom tertentu (disini kolom : 'C')
df.dropna(subset=['C'])
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.0</td>
      <td>NaN</td>
      <td>12.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<p>Menghilangkan data mungkin tidak diharapkan karena data akan menjadi lebih sedikit, oleh karena itu perlu penyelesaian data hilang dengan cara <u>mengimputkan </u> data. Teknik ini disebut dengan teknik inputer. Salah satunya dengan menggantikan dengan nilai statistik</p>

<pre><code class="language-python">from sklearn.preprocessing import Imputer

# pilihan dari library imputer terdiri dari niai  mean, median, modus ( nilai yang paling dering/most_frequent)

imr = Imputer(missing_values='NaN', strategy='mean', axis=0)
imr = imr.fit(df)
imputed_data = imr.transform(df.values)
imputed_data
</code></pre>

<pre><code>array([[ 1. ,  2. ,  3. ,  4. ],
       [ 5. ,  2. ,  7.5,  8. ],
       [10. ,  2. , 12. ,  6. ]])
</code></pre>

<p>Misalkan diatas  7.5 diatas rata-rata dari 3 dan 12. Dan 6 adalah rata rata dari 4 dan 8.</p>

<pre><code class="language-python">df.values
</code></pre>

<pre><code>array([[  1.,   2.,   3.,   4.],
       [  5.,   6.,  nan,   8.],
       [ 10.,  11.,  12.,  nan]])
</code></pre>

<p>Kita dapat melakukan ini lebih baik dari ini dengan memilih hanya baris/obek yang lebih mirip untuk interpolasi, dibandingkan jika memilih semua baris. Ini bagaiman sistem rekomendasi dapat bekerja yaitu memprediksi rating dari suatu film atau buku. Silahkan baca buku
<i>Programming Collective Intelligence: Building Smart Web 2.0 Applications, by Toby Segaran</i>
* buku ini sangat baik untuk sistem rekomendasi dan  dan  search engine.
<a href="https://www.amazon.com/Programming-Collective-Intelligence-Building-Applications/dp/0596529325/ref=sr_1_1?s=books&ie=UTF8&qid=1475564389&sr=1-1&keywords=collective+intelligence">
<img src="https://images-na.ssl-images-amazon.com/images/I/51LolW3DugL._SX379_BO1,204,203,200_.jpg" width=25% align=right>
</a></p>

<h2 id="understanding-the-scikit-learn-estimator-api">Understanding the scikit-learn estimator API</h2>

<p>Transformer class for data transformation
* imputer</p>

<p>Key methods
* fit() for fitting from (training) ata
* transform() for transforming future data based on the fitted data</p>

<p>Good API designs are consistent. For example, the fit() method has similar meanings for different classes, such as transformer and estimator.</p>

<h3 id="transformer">Transformer</h3>

<p><img src='./images/04_04.png' width=80%></p>

<h3 id="estimator">Estimator</h3>

<p><img src='./images/04_05.png' width=80%></p>

<h1 id="mengatasi-beberapa-macam-bentuk-tipe-data">Mengatasi beberapa macam bentuk tipe data</h1>

<p>Ada beberapa bentuk tipe fitur data, yaitu : numerik dan kategorikal.
Fitur numerik adalah bilangan dan sering kontine seperti bilangan riel.
Fitur Kategorikal adalah diskrit dan berupa tipe nominal atau ordinal
* Nilai ordinal adalah diskrit tetapi mempunyai makan numerik sehingga dapat diurutkan atau bermakna tingkatan
* Nilai nominal tidak mempunyai makna numerik.</p>

<p>Dalam conoth berikut:
* warna adalah tipe nominal ( tidak punya makna numerik)
* ukuran adalah tipe ordinal ( dapat diurutkan )
* harga adalah tipe numerik
Suat data dapat berisi titpe data yang berbeda-beda .Oleh karena itu kita sangat penting untuk menangani secara hati hati. Kita tidak dapat memperlakukan nilai nominal sebagi numerik tanpa memetakan tipe tersebut sebagai mana mestinya</p>

<pre><code class="language-python">import pandas as pd

df = pd.DataFrame([['green', 'M', 10.1, 'kelas1'],
                   ['red', 'L', 13.5, 'kelas2'],
                   ['blue', 'XL', 15.3, 'kelas1']])

df.columns = ['warna', 'ukuran', 'harga', 'kelas']
df
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>warna</th>
      <th>ukuran</th>
      <th>harga</th>
      <th>kelas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>M</td>
      <td>10.1</td>
      <td>kelas1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>L</td>
      <td>13.5</td>
      <td>kelas2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>XL</td>
      <td>15.3</td>
      <td>kelas1</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="konversi-data">Konversi data</h2>

<p>Untuk beberapa metode klasifikasi misalkan pohon keputusan yang menangani satu fitur satu waktu. Tidak ada masalah jika kita tidak melakukan konversi fitur dari fitur yang ada.</p>

<p>Akan tetapi untuk metode klasifikasi yang lain, butuh untuk menangani beberapa fitur bersama, kita perlu mengkonversinya ke bentuk yang sesuai sebelum diproses lebih lanjut.:
1. mengkonversi nilai kategorikal ke nilai numerik
2. menskalakan/ menormalisasi nilai numerik</p>

<h2 id="pemetaan-fitur-ordinal">Pemetaan fitur ordinal</h2>

<p>Fitur Ordinal dapat dikonversi ke bilangan, tetapi pengkonversian selalu bergantung pada semantik dan kemudian perlu secara manual ditentukan oleh orang kemudian digantikan secara otomatis oleh mesin.</p>

<p>Dalam contoh berikut, kita dapat memetakan ukuran ke bilangan. Biasanya ukuran besar dinyatakan dengan angka yang besar.</p>

<p>Contoh berikut, kita gunakan kamus python untuk mendefinisikan pemetaan</p>

<pre><code class="language-python">pemetaan_ukuran = {'XL': 3,
                'L': 2,
                'M': 1}

df['ukuran'] = df['ukuran'].map(pemetaaan_ukuran)
df
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>warna</th>
      <th>ukuran</th>
      <th>harga</th>
      <th>kelas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>kelas1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>kelas2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>kelas1</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">inv_petaaan_ukuran = {v: k for k, v in pemetaaan_ukuran.items()}
df['ukuran'].map(inv_petaaan_ukuran)
</code></pre>

<pre><code>0     M
1     L
2    XL
Name: ukuran, dtype: object
</code></pre>

<h2 id="mengkodekan-label-kelas">Mengkodekan label kelas</h2>

<p>Label Kelas sering perlu dinyatakan sebagai bilangan bulat dalam pustaka pembelajaran mesin
* biasanya menggunakan bilangan bilangan kecil seperti 0,1, 2 dan seterusnya1, &hellip;
* bukan ordinal</p>

<pre><code class="language-python">import numpy as np

pemetaan_kelas = {label: idx for idx, label in enumerate(np.unique(df['kelas']))}
pemetaan_kelas
</code></pre>

<pre><code>{0: 0, 1: 1}
</code></pre>

<pre><code class="language-python"># forward map
df['kelas'] = df['kelas'].map(pemetaan_kelas)
df
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>warna</th>
      <th>ukuran</th>
      <th>harga</th>
      <th>kelas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># inverse dari peta kelas
inv_pemetaan_kelas = {v: k for k, v in pemetaan_kelas.items()}
df['kelas'] = df['kelas'].map(inv_pemetaan_kelas)
df
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>warna</th>
      <th>ukuran</th>
      <th>harga</th>
      <th>kelas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Kita dapat menggunakan  <strong>LabelEncoder</strong> dalam  scikit learn untuk mengkonversi label kelas secara otomatis</p>

<pre><code class="language-python">from sklearn.preprocessing import LabelEncoder

class_le = LabelEncoder()
y = class_le.fit_transform(df['kelas'].values)
df['kelas'] = y
df
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>warna</th>
      <th>ukuran</th>
      <th>harga</th>
      <th>kelas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">class_le.inverse_transform(y)
</code></pre>

<pre><code>array(['class1', 'class2', 'class1'], dtype=object)
</code></pre>

<h2 id="melakukan-one-hot-encoding-pada-fitur-nominal">Melakukan  <strong><em>one-hot encoding</em></strong> pada  fitur nominal</h2>

<p>Akan tetapi, tidak seperti dilakukan pada label kelas, kita tidak dapat hanya mengkonversi fitur nominal ( seperti warna ) secara langsung ke integer.</p>

<p>Kesalahan yang sering dilakukan adalah memetakan fitur nominal langsung ke nilai numerik misalkan untuk warna common mistake is to map nominal features into numerical values, e.g. for colors
* biru $\rightarrow$ 0
* hijau $\rightarrow$ 1
* merah $\rightarrow$ 2</p>

<pre><code class="language-python">X = df[['warna', 'ukuran', 'harga']].values

color_le = LabelEncoder()
X[:, 0] = color_le.fit_transform(X[:, 0])
X
</code></pre>

<pre><code>array([[1, 1, 10.1],
       [2, 2, 13.5],
       [0, 3, 15.3]], dtype=object)
</code></pre>

<p>Untuk fitur kategorikal, yang penting adalah memperthanakan nilai &ldquo;  &ldquo;equal distance&rdquo;
* keculai anda punya alasan lain
Misalkan, untuk warna meraah, hijau, biru, kita ingin mengkonversi nya ke nilai sehingga masing masing warna memiliki jarak sama satu dengan yang lain.</p>

<p>Ini tidak dapat dilakukan  dalam 1D tetapi dapat dilakukan dalam 2D</p>

<p>One hot encoding adalah cara yang tepat untuk melakukan ini dengan memetakan  n-nilai fitur nominal ke n-dimensi vektor biner.
* biru  $\rightarrow$ (1, 0, 0)
* hijau $\rightarrow$ (0, 1, 0)
* merah   $\rightarrow$ (0, 0, 1)</p>

<pre><code class="language-python">from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(categorical_features=[0])
ohe.fit_transform(X).toarray()
</code></pre>

<pre><code>array([[ 0. ,  1. ,  0. ,  1. , 10.1],
       [ 0. ,  0. ,  1. ,  2. , 13.5],
       [ 1. ,  0. ,  0. ,  3. , 15.3]])
</code></pre>

<pre><code class="language-python"># secara otomatis denga metode get_dummies dalamp pandas (pd)
pd.get_dummies(df[['harga', 'warna', 'ukuran']])
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>harga</th>
      <th>ukuran</th>
      <th>warna_blue</th>
      <th>warna_green</th>
      <th>warna_red</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10.1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13.5</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15.3</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>warna</th>
      <th>ukuran</th>
      <th>harga</th>
      <th>kelas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="binning">Binning</h1>

<p>Often we need to do the reverse of what we&rsquo;ve done above. That is, convert continuous features to discrete values. For instance, we want to convert the output to 0 or 1 depending on the threshold.</p>

<pre><code class="language-python">from sklearn.datasets import load_iris

iris_dataset = load_iris()
X = iris_dataset.data
y = iris_dataset.target
feature_names = iris_dataset.feature_names
</code></pre>

<p>Now we&rsquo;ll binarize the sepal width with 0 or 1 indicating whether the current value is below or above mean.</p>

<pre><code class="language-python">X[:, 1]
</code></pre>

<pre><code>array([3.5, 3. , 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3. ,
       3. , 4. , 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3. ,
       3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3. ,
       3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3. , 3.8, 3.2, 3.7, 3.3, 3.2, 3.2,
       3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2. , 3. , 2.2, 2.9, 2.9,
       3.1, 3. , 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3. , 2.8, 3. ,
       2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3. , 3.4, 3.1, 2.3, 3. , 2.5, 2.6,
       3. , 2.6, 2.3, 2.7, 3. , 2.9, 2.9, 2.5, 2.8, 3.3, 2.7, 3. , 2.9,
       3. , 3. , 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3. , 2.5, 2.8, 3.2, 3. ,
       3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3. , 2.8, 3. ,
       2.8, 3.8, 2.8, 2.8, 2.6, 3. , 3.4, 3.1, 3. , 3.1, 3.1, 3.1, 2.7,
       3.2, 3.3, 3. , 2.5, 3. , 3.4, 3. ])
</code></pre>

<pre><code class="language-python">from sklearn.preprocessing import Binarizer
X[:, 1:2] = Binarizer(threshold=X[:, 1].mean()).fit_transform(X[:, 1].reshape(-1, 1))
X[:, 1]
</code></pre>

<pre><code>array([1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,
       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.])
</code></pre>

<h1 id="membagi-data-ke-dalam-data-pelatihan-dan-data-uji">Membagi data ke dalam data pelatihan dan data uji</h1>

<p>Data  pelatihan untuk melatih model</p>

<p>Data uji untuk mengevaluasi model yang telah dilatih</p>

<p>Membagi menjadi dua untuk menghindari  over-fitting yitu model baik secara umum untuk data uji
* well trained models should generalize to unseen, test data</p>

<p>Validation set untuk memilih  hyper-parameter
* parameters yang telah dilatih oleh algoritma
* hyper-parameters dipilih oleh orang
* will talk this later you do&rsquo;not worry i will explain that more clearly at next chapter</p>

<h2 id="data-anggur">Data Anggur</h2>

<p>Data yang dipakai untuk mengklasifikasikan anggur berdasarkan 13 fitur dan ada 178 data.</p>

<pre><code class="language-python">#wine_data_remote = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'
import numpy as np
import pandas as pd
wine_data_local = 'M:/Dataset\Machine learning/pyml-master/code/datasets/wine/wine.data'
#wine_data_local = '../datasets/wine/wine.data'

df_wine = pd.read_csv(wine_data_local,
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

print('Class labels', np.unique(df_wine['Class label']))
df_wine.head()
</code></pre>

<pre><code>Class labels [1 2 3]
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class label</th>
      <th>Alcohol</th>
      <th>Malic acid</th>
      <th>Ash</th>
      <th>Alcalinity of ash</th>
      <th>Magnesium</th>
      <th>Total phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diluted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="bagaimana-menentukan-proporsi-dari-data-pelatihan-dan-data-uji">Bagaimana menentukan proporsi dari data pelatihan dan data uji?</h2>

<p>Seberapa banyak data pelatihan memungkinkan untuk akurasi model</p>

<p>Seberapa banyak data uji yang memungkin untuk evaluasi</p>

<p>Aturan umumnya adalah 60:40, 70:30, 80:20</p>

<p>Semakin besar data semakin banyak proporsi untuk data pelatihan  datasets can have more portions for training
* yaitu. 90:10</p>

<p>kemungkinan kemungkinan pembagian lain nanti akan dibahas dalam bab dalam evaluasi model dan memilih parameter</p>

<pre><code class="language-python">if Version(sklearn_version) &lt; '0.18':
    from sklearn.cross_validation import train_test_split
else:
    from sklearn.model_selection import train_test_split

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.3, random_state=0)
    
print(X.shape)

import numpy as np
print(np.unique(y))
</code></pre>

<pre><code>(178, 13)
[1 2 3]
</code></pre>

<h1 id="menjadikan-fitur-fitur-pada-skala-yang-sama">Menjadikan fitur fitur pada skala yang sama</h1>

<p>Sebagian besar algoritma-algoritma pembelajaran mesin menjadi  lebih baik ketika fitur fiturnya pada skala yang sama</p>

<p>Kecuali
* Pohon keputusan/decision tree
* Random forest</p>

<h2 id="contoh">Contoh</h2>

<p>Dua fitur, dalam sakal [0 1] dan [0 100000]</p>

<p>Coba bayangkan kira kira apa yang terjadi ketiak kita menggunakan
* perceptron
* KNN</p>

<h2 id="ada-dua-pendekatan-umum">Ada dua pendekatan umum</h2>

<h3 id="nomalisasi">Nomalisasi</h3>

<p>Skala min-max :
$$\frac{x-x<em>{min}}{x</em>{max}-x_{min}}$$</p>

<h3 id="standarisiasi">Standarisiasi</h3>

<p>Skala standar:
$$\frac{x-x<em>\mu}{x</em>\sigma}$$
* $x<em>\mu$: rata rata dari nilai-nilai x
* $x</em>\sigma$: standar deviasi dari nilai-nilai x</p>

<p>Standarisiasi lebih umum dari normalisasi karena normalisasi sensitif atau peka terhada data outlier</p>

<pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
</code></pre>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
</code></pre>

<h2 id="contoh-kasus-standarisasi-dan-normalisasi">Contoh kasus: standarisasi  dan normalisasi</h2>

<pre><code class="language-python">import pandas as pd
ex = pd.DataFrame([0, 1, 2, 3, 4, 5])

# standarisasi
ex[1] = (ex[0] - ex[0].mean()) / ex[0].std(ddof=0)

# Silahkan anda perhatikan bahwa pandas menggunakan ddof=1 (standar deviasi sample ) 
# dengan default, dimana metode NumPy's std  dan  StandardScaler
# menggunakan ddof=0 (standar deviasi populasi )

# normalisasi
ex[2] = (ex[0] - ex[0].min()) / (ex[0].max() - ex[0].min())
ex.columns = ['input', 'standardisasi', 'normalisasi']
ex
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>input</th>
      <th>standardisasi</th>
      <th>normalisasi</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>-1.46385</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-0.87831</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.29277</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.29277</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.87831</td>
      <td>0.8</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>1.46385</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="memilih-fitur-penting">Memilih fitur penting</h1>

<p>Overfitting adalah masalah umum pada pemebelajaran mesin.
* model sangat sesuai untuk training data and gagal  secara umum untuk data riel
* model terlalu komplek untuk data pelatihan yang diberikan</p>

<p><img src="./images/03_06.png" width=80%></p>

<h2 id="cara-mengatasi-overfitting">Cara mengatasi overfitting</h2>

<ul>
<li>Kumpulkan lebih banyak data pelatihan (untuk mengurangi overfitting)</li>
<li>Mengurangi kompleksitas model, misalkan banyak parameter</li>
<li>Mengurangi kompleksitas model secara tidak langsung melalui aturan aturan (regularization)</li>
<li>Mengurangi dimensi data, yang akan memaksa mereduksi suatu model</li>
</ul>

<p>Jumlah data harus cukup untuk memodelkan kompleksitas</p>

<h2 id="objective">Objective</h2>

<p>Kita dapat menjumlahkan faktor loss dan regularization sebagai total objective:
$$\Phi(\mathbf{X}, \mathbf{T}, \Theta) = L\left(\mathbf{X}, \mathbf{T}, \mathbf{Y}=f(\mathbf{X}, \Theta)\right) + P(\Theta)$$</p>

<p>Selama pelatihan, tujuannya adlah mengoptimalkan  parameter $\Theta$ terhadap data pelatihan yang diberikan $\mathbf{X}$ and $\mathbf{T}$:
$$argmin_\Theta \; \Phi(\mathbf{X}, \mathbf{T}, \Theta)$$
dan berharap model setelah dilatih secara umum baik untuk data nantinya.</p>

<h2 id="loss">Loss</h2>

<p>Setiap tugas pembelajaran mesin  sebagai tujuan yang dapt dinyatakan sebagai fungsi loss :
$$L(\mathbf{X}, \mathbf{T}, \mathbf{Y})$$
, dimana $\mathbf{T}$  adalah  bentuk dari target atau informasi tambahan, seperti:
* label untuk klasifikasi terawasi
* jumlah klaster untuk pembelajaran tak terawasi (unsupervised clustering)
* lingkungan untuk reinforcement learning</p>

<h2 id="regularization">Regularization</h2>

<p>Selanjutnya untuk objective, kita sering  digunakan untuk menyederhanakan suatu model, untuk lebih mengefisienkan dan mengernarilasis  (menghindari over-fitting).
Kompleksitas dari suatu model dapat diukur dengan fungsi penalty :
$$P(\Theta)$$
Beberapa fungsi penalty terdiri dari bilangan dan atau  besaran parameter</p>

<h2 id="regularization-1">Regularization</h2>

<p>Untuk vektor bobot $\mathbf{w}$ dari suatu model  (misalkan. perceptron atau SVM)</p>

<p>$L_2$:
$
|\mathbf{w}|<em>2^2 = \sum</em>{k} w_k^2
$</p>

<p>$L_1$:
$
|\mathbf{w}|<em>1 = \sum</em>{k} \left|w_k \right|
$</p>

<p>$L_1$ lebih cenderung menghasilakan solusi yang sparse daripada $L_2$
* banyak  bobot nol
* lebih mirip seleksi fitur</p>

<p><img src='./images/04_12.png' width=80%></p>

<p><img src='./images/04_13.png' width=80%></p>

<p>We are more likely to bump into sharp corners of an object.</p>

<p>Experiment: drop a circle and a square into a flat floor.
What is the probability of hitting any point on the shape?</p>

<p><img src="./images/sharp_and_round.svg" width=80%></p>

<p>How about a non-flat floor, e.g. concave or convex with different curvatures?</p>

<h2 id="regularization-in-scikit-learn">Regularization in scikit-learn</h2>

<p>Many ML models support regularization with different
* methods (e.g. $L_1$ and $L_2$)
* strength (the $C$ value inversely proportional to regularization strength)</p>

<pre><code class="language-python">import warnings
warnings.filterwarnings('ignore')
from sklearn.linear_model import LogisticRegression

# l1 regularization
lr = LogisticRegression(penalty='l1', C=0.1)
lr.fit(X_train_std, y_train)

# compare training and test accuracy to see if there is overfitting
print('Training accuracy:', lr.score(X_train_std, y_train))
print('Test accuracy:', lr.score(X_test_std, y_test))
</code></pre>

<pre><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-4-1346b47a2850&gt; in &lt;module&gt;()
      5 # l1 regularization
      6 lr = LogisticRegression(penalty='l1', C=0.1)
----&gt; 7 lr.fit(X_train_std, y_train)
      8 
      9 # compare training and test accuracy to see if there is overfitting


NameError: name 'X_train_std' is not defined
</code></pre>

<pre><code class="language-python"># 3 sets of parameters due to one-versus-rest with 3 classes
lr.intercept_
</code></pre>

<pre><code>array([-0.38379411, -0.15807589, -0.70040327])
</code></pre>

<pre><code class="language-python"># 13 coefficients for 13 wine features; notice many of them are 0
lr.coef_
</code></pre>

<pre><code>array([[ 0.28031365,  0.        ,  0.        , -0.02824937,  0.        ,
         0.        ,  0.70991702,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  1.23620609],
       [-0.64400971, -0.06874145, -0.05721952,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        , -0.9267147 ,
         0.06019752,  0.        , -0.37102696],
       [ 0.        ,  0.06145033,  0.        ,  0.        ,  0.        ,
         0.        , -0.63649346,  0.        ,  0.        ,  0.4983057 ,
        -0.3581246 , -0.57078233,  0.        ]])
</code></pre>

<pre><code class="language-python">import warnings
warnings.filterwarnings('ignore')
from sklearn.linear_model import LogisticRegression

# l2 regularization
lr = LogisticRegression(penalty='l2', C=0.1)
lr.fit(X_train_std, y_train)

# compare training and test accuracy to see if there is overfitting
print('Training accuracy:', lr.score(X_train_std, y_train))
print('Test accuracy:', lr.score(X_test_std, y_test))
</code></pre>

<pre><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-2-3e29b8113667&gt; in &lt;module&gt;()
      5 # l2 regularization
      6 lr = LogisticRegression(penalty='l2', C=0.1)
----&gt; 7 lr.fit(X_train_std, y_train)
      8 
      9 # compare training and test accuracy to see if there is overfitting


NameError: name 'X_train_std' is not defined
</code></pre>

<pre><code class="language-python"># notice the disappearance of 0 coefficients due to L2
lr.coef_
</code></pre>

<pre><code>array([[ 0.58228361,  0.04305595,  0.27096654, -0.53333363,  0.00321707,
         0.29820868,  0.48418851, -0.14789735, -0.00451997,  0.15005795,
         0.08295104,  0.38799131,  0.80127898],
       [-0.71490217, -0.35035394, -0.44630613,  0.32199115, -0.10948893,
        -0.03572165,  0.07174958,  0.04406273,  0.20581481, -0.71624265,
         0.39941835,  0.17538899, -0.72445229],
       [ 0.18373457,  0.32514838,  0.16359432,  0.15802432,  0.09025052,
        -0.20530058, -0.53304855,  0.1117135 , -0.21005439,  0.62841547,
        -0.4911972 , -0.55819761, -0.04081495]])
</code></pre>

<h2 id="plot-regularization">Plot regularization</h2>

<p>$C$ is inverse to the regularization strength</p>

<pre><code class="language-python">import warnings
warnings.filterwarnings('ignore') # menghilangkan warning dari jupyter notebook

import matplotlib.pyplot as plt

fig = plt.figure()
ax = plt.subplot(111)
    
colors = ['blue', 'green', 'red', 'cyan', 
          'magenta', 'yellow', 'black', 
          'pink', 'lightgreen', 'lightblue', 
          'gray', 'indigo', 'orange']

weights, params = [], []
for c in np.arange(-4, 6,dtype  = float):
    lr = LogisticRegression(penalty='l1', C=10**c, random_state=0)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])
    params.append(10**c)

weights = np.array(weights)

for column, color in zip(range(weights.shape[1]), colors):
    plt.plot(params, weights[:, column],
             label=df_wine.columns[column + 1],
             color=color)
plt.axhline(0, color='black', linestyle='--', linewidth=3)
plt.xlim([10**(-5), 10**5])
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.xscale('log')
plt.legend(loc='upper left')
ax.legend(loc='upper center', 
          bbox_to_anchor=(1.38, 1.03),
          ncol=1, fancybox=True)
# plt.savefig('./figures/l1_path.png', dpi=300)
plt.show()
</code></pre>

<pre><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-3-21815fe97d12&gt; in &lt;module&gt;()
     13 
     14 weights, params = [], []
---&gt; 15 for c in np.arange(-4, 6,dtype  = float):
     16     lr = LogisticRegression(penalty='l1', C=10**c, random_state=0)
     17     lr.fit(X_train_std, y_train)


NameError: name 'np' is not defined
</code></pre>

<p><img src="output_77_1.png" alt="png" /></p>

<h1 id="dimensionality-reduction">Dimensionality reduction</h1>

<p>$L_1$ regularization implicitly selects features via zero out</p>

<p>Feature selection
* explicit - you specify how many features to select, the algorithm picks the most relevant (not important) ones
* forward, backward
* next topic</p>

<p>Note: 2 important features might be highly correlated, and thus it is relevant to select only 1</p>

<p>Feature extraction
* implicit
* can build new, not just select original, features
* e.g. PCA
* next chapter</p>

<h2 id="algoritm-seleksi-fitur-sequential">Algoritm seleksi fitur Sequential</h2>

<p>Seleksi fitur adalah cara mengurangi dimensid data. Anda bisa mengatakan juga seleksi fitur mengurangi jumlah kolom dari data
Bagaiman kita memutuskan fitur mana yang akan dipertahankan. Tentunya, kita ingin mempertahankan fitur fitur yang relevan dan membuang yang tidak relevan.</p>

<p>Kita dapat memilih fitur ini secara berurutan apakah secara maju atau mundur (backward atau backward)</p>

<h3 id="seleksi-mundur-backward-selection">Seleksi mundur  (Backward Selection)</h3>

<p>Seleksi fitur mundur secara berurutan (Sequential backward selection /(SBS) adalah menyelidik diri sendiri. Ide dasarnya adalah mulai dengan $n$ fitur, dan  mempertahankan semua kemungkindan  $n-1$ subfitur, dan menghapus satu yang paling penting untuk model pelatihan.</p>

<p>Kemudian kita berpindah untuk mengurangi jumlah fitur selanjutnya ($[n-2, n-3, \cdots]$) sampai jumlah fitur yang diharapkan</p>

<pre><code class="language-python">from sklearn.base import clone
from itertools import combinations
import numpy as np
from sklearn.metrics import accuracy_score
if Version(sklearn_version) &lt; '0.18':
    from sklearn.cross_validation import train_test_split
else:
    from sklearn.model_selection import train_test_split


class SBS():
    def __init__(self, estimator, k_features, scoring=accuracy_score,
                 test_size=0.25, random_state=1):
        self.scoring = scoring
        self.estimator = clone(estimator)
        self.k_features = k_features
        self.test_size = test_size
        self.random_state = random_state

    def fit(self, X, y):
        
        X_train, X_test, y_train, y_test = \
            train_test_split(X, y, test_size=self.test_size,
                             random_state=self.random_state)

        dim = X_train.shape[1]
        self.indices_ = tuple(range(dim))
        self.subsets_ = [self.indices_]
        score = self._calc_score(X_train, y_train, 
                                 X_test, y_test, self.indices_)
        self.scores_ = [score]

        while dim &gt; self.k_features:
            scores = []
            subsets = []

            for p in combinations(self.indices_, r=dim - 1):
                score = self._calc_score(X_train, y_train, 
                                         X_test, y_test, p)
                scores.append(score)
                subsets.append(p)

            best = np.argmax(scores)
            self.indices_ = subsets[best]
            self.subsets_.append(self.indices_)
            dim -= 1

            self.scores_.append(scores[best])
        self.k_score_ = self.scores_[-1]

        return self

    def transform(self, X):
        return X[:, self.indices_]

    def _calc_score(self, X_train, y_train, X_test, y_test, indices):
        self.estimator.fit(X_train[:, indices], y_train)
        y_pred = self.estimator.predict(X_test[:, indices])
        score = self.scoring(y_test, y_pred)
        return score
</code></pre>

<p>Below we try to apply the SBS class above.
We use the KNN classifer, which can suffer from curse of dimensionality.</p>

<pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=2)

# selecting features
sbs = SBS(knn, k_features=1)
sbs.fit(X_train_std, y_train)

# plotting performance of feature subsets
k_feat = [len(k) for k in sbs.subsets_]

plt.plot(k_feat, sbs.scores_, marker='o')
plt.ylim([0.7, 1.1])
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.grid()
plt.tight_layout()
# plt.savefig('./sbs.png', dpi=300)
plt.show()
</code></pre>

<pre><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-10-a121ec32336f&gt; in &lt;module&gt;()
      6 # selecting features
      7 sbs = SBS(knn, k_features=1)
----&gt; 8 sbs.fit(X_train_std, y_train)
      9 
     10 # plotting performance of feature subsets


NameError: name 'X_train_std' is not defined
</code></pre>

<pre><code class="language-python"># list the 5 most important features
k5 = list(sbs.subsets_[8]) # 5+8 = 13
print(df_wine.columns[1:][k5])
</code></pre>

<pre><code>Index(['Alcohol', 'Malic acid', 'Alcalinity of ash', 'Hue', 'Proline'], dtype='object')
</code></pre>

<pre><code class="language-python">knn.fit(X_train_std, y_train)
print('Training accuracy:', knn.score(X_train_std, y_train))
print('Test accuracy:', knn.score(X_test_std, y_test))
</code></pre>

<pre><code>Training accuracy: 0.983870967742
Test accuracy: 0.944444444444
</code></pre>

<pre><code class="language-python">knn.fit(X_train_std[:, k5], y_train)
print('Training accuracy:', knn.score(X_train_std[:, k5], y_train))
print('Test accuracy:', knn.score(X_test_std[:, k5], y_test))
</code></pre>

<pre><code>Training accuracy: 0.959677419355
Test accuracy: 0.962962962963
</code></pre>

<p>Note the improved test accuracy by fitting lower dimensional training/test data.</p>

<h3 id="forward-selection">Forward selection</h3>

<p>This is essetially the reverse of backward selection; we will leave this as an exercise.</p>

<h1 id="assessing-feature-importances-with-random-forests">Assessing Feature Importances with Random Forests</h1>

<p>Recall
* a decision tree is built by splitting nodes
* each node split is to maximize information gain
* random forest is a collection of decision trees with randomly selected features</p>

<p>Information gain (or impurity loss) at each node can measure the importantce of the feature being split</p>

<pre><code class="language-python"># feature_importances_ from random forest classifier records this info
from sklearn.ensemble import RandomForestClassifier

feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=10000,
                                random_state=0,
                                n_jobs=-1)

forest.fit(X_train, y_train)
importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):
    print(&quot;%2d) %-*s %f&quot; % (f + 1, 30, 
                            feat_labels[indices[f]], 
                            importances[indices[f]]))

plt.title('Feature Importances')
plt.bar(range(X_train.shape[1]), 
        importances[indices],
        color='lightblue', 
        align='center')

plt.xticks(range(X_train.shape[1]), 
           feat_labels[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
#plt.savefig('./random_forest.png', dpi=300)
plt.show()
</code></pre>

<pre><code> 1) Color intensity                0.182483
 2) Proline                        0.158610
 3) Flavanoids                     0.150948
 4) OD280/OD315 of diluted wines   0.131987
 5) Alcohol                        0.106589
 6) Hue                            0.078243
 7) Total phenols                  0.060718
 8) Alcalinity of ash              0.032033
 9) Malic acid                     0.025400
10) Proanthocyanins                0.022351
11) Magnesium                      0.022078
12) Nonflavanoid phenols           0.014645
13) Ash                            0.013916
</code></pre>

<p><img src="output_90_1.png" alt="png" /></p>

<pre><code class="language-python">threshold = 0.15
if False: #Version(sklearn_version) &lt; '0.18':
    X_selected = forest.transform(X_train, threshold=threshold)
else:
    from sklearn.feature_selection import SelectFromModel
    sfm = SelectFromModel(forest, threshold=threshold, prefit=True)
    X_selected = sfm.transform(X_train)

X_selected.shape
</code></pre>

<pre><code>(124, 3)
</code></pre>

<p>Now, let&rsquo;s print the 3 features that met the threshold criterion for feature selection that we set earlier (note that this code snippet does not appear in the actual book but was added to this notebook later for illustrative purposes):</p>

<pre><code class="language-python">for f in range(X_selected.shape[1]):
    print(&quot;%2d) %-*s %f&quot; % (f + 1, 30, 
                            feat_labels[indices[f]], 
                            importances[indices[f]]))
</code></pre>

<pre><code> 1) Color intensity                0.182483
 2) Proline                        0.158610
 3) Flavanoids                     0.150948
</code></pre>

<h1 id="summary">Summary</h1>

<p>Data is important for machine learning: garbage in, garbage out.
So pre-process data is important.
This chapter covers various topics for data processing, such as handling missing data, treating different types of data (numerical, categorical), and how to avoid over-fitting which can improve both accuracy and speed.</p>

<h1 id="reading">Reading</h1>

<ul>
<li>PML Chapter 4</li>
<li>IML Chapter 6.2</li>
</ul>

		</div>
		
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="   - Mulaab - avatar" src="https://avatars3.githubusercontent.com/u/4085973?s=460&amp;v=4" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About    - Mulaab -</span>
	</div>
	<div class="authorbox__description">
		I am lecture at Trunojoyo University.
	</div>
</div>

<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="https://mulaab.github.io/blog/post/menjadi-penulis-ieee-yang-profesional/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title"></p></a>
	</div>
</nav>


			</div>
			<aside class="sidebar">
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/210519-basic-elements/">Menulis bukan bakatku...</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/220519-penulis-terbaik/">Menjadi Penulis Terbaik</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/creating-a-new-theme/">Creating a New Theme</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/200519-migrate-from-jekyll/">Migrate to Hugo from Jekyll</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/190519-hugois/">(Hu)go Template Primer</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/hugoisforlovers/">Getting Started with Hugo</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/menjadi-penulis-ieee-yang-profesional/"></a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/post/praproses-data/"></a></li>
		</ul>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/categories/development">Development</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/categories/golang">Golang</a></li>
			<li class="widget__item"><a class="widget__link" href="https://mulaab.github.io/blog/categories/pribadi">Pribadi</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/basic-elements" title="Basic elements">Basic elements</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/css" title="Css">Css</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/dasar" title="Dasar">Dasar</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/development" title="Development">Development</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/go" title="Go">Go</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/golang" title="Golang">Golang</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/html" title="Html">Html</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/hugo" title="Hugo">Hugo</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/templates" title="Templates">Templates</a>
		<a class="widget-taglist__link widget__link btn" href="https://mulaab.github.io/blog/tags/themes" title="Themes">Themes</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 @M0ula__ab.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="https://mulaab.github.io/blog/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>